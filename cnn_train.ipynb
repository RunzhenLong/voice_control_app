{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wave\n",
    "import pylab\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Set paths to input and output data\n",
    "INPUT_DIR = 'free_spoken_digit_dataset_master/recordings/'\n",
    "OUTPUT_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "\n",
      "class_0\n",
      "class_1\n",
      "class_2\n",
      "class_3\n",
      "class_4\n",
      "class_5\n",
      "class_6\n",
      "class_7\n",
      "class_8\n",
      "class_9\n",
      "\n",
      "A few example files: \n",
      "\n",
      "1_george_0.png\n",
      "1_george_1.png\n",
      "1_george_10.png\n",
      "1_george_11.png\n",
      "1_george_12.png\n",
      "1_george_13.png\n",
      "1_george_14.png\n",
      "1_george_15.png\n",
      "1_george_16.png\n",
      "1_george_17.png\n"
     ]
    }
   ],
   "source": [
    "# Utility function to get sound and frame rate info\n",
    "def get_wav_info(wav_file):\n",
    "    wav = wave.open(wav_file, 'r')\n",
    "    frames = wav.readframes(-1)\n",
    "    sound_info = pylab.frombuffer(frames, 'int16')\n",
    "    frame_rate = wav.getframerate()\n",
    "    wav.close()\n",
    "    return sound_info, frame_rate\n",
    "\n",
    "\"\"\"\n",
    "# For every recording, make a spectogram and save it as label_speaker_no.png\n",
    "if not os.path.exists(os.path.join(OUTPUT_DIR, 'audio-images')):\n",
    "    os.mkdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n",
    "    \n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if \"wav\" in filename:\n",
    "        file_path = os.path.join(INPUT_DIR, filename)\n",
    "        file_stem = Path(file_path).stem\n",
    "        target_dir = f'class_{file_stem[0]}'\n",
    "        dist_dir = os.path.join(os.path.join(OUTPUT_DIR, 'audio-images'), target_dir)\n",
    "        file_dist_path = os.path.join(dist_dir, file_stem)\n",
    "        if not os.path.exists(file_dist_path + '.png'):\n",
    "            if not os.path.exists(dist_dir):\n",
    "                os.mkdir(dist_dir)\n",
    "            file_stem = Path(file_path).stem\n",
    "            sound_info, frame_rate = get_wav_info(file_path)\n",
    "            plt.specgram(sound_info, Fs=frame_rate)\n",
    "            plt.axis('off')\n",
    "            plt.savefig(f'{file_dist_path}.png', bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "# Print the ten classes in our dataset\n",
    "path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n",
    "print(\"Classes: \\n\")\n",
    "for i in range(10):\n",
    "    print(path_list[i])\n",
    "    \n",
    "# File names for class 1\n",
    "path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images/class_1'))\n",
    "print(\"\\nA few example files: \\n\")\n",
    "for i in range(10):\n",
    "    print(path_list[i])\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 files belonging to 10 classes.\n",
      "Using 2400 files for training.\n",
      "Found 3000 files belonging to 10 classes.\n",
      "Using 600 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Declare constants\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "BATCH_SIZE = 32\n",
    "N_CHANNELS = 3\n",
    "N_CLASSES = 10\n",
    "\n",
    "# Make a dataset containing the training spectrograms\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"training\",\n",
    "                                             seed=0)\n",
    "\n",
    "# Make a dataset containing the validation spectrogram\n",
    "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"validation\",\n",
    "                                             seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(ds, augment=False):\n",
    "    # Define our one transformation\n",
    "    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n",
    "    flip_and_rotate = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "    ])\n",
    "    \n",
    "    # Apply rescale to both datasets and augmentation only to training\n",
    "    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n",
    "    if augment: ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n",
    "    return ds\n",
    "\n",
    "train_dataset = prepare(train_dataset, augment=False)\n",
    "valid_dataset = prepare(valid_dataset, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               8388864   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 8,487,498\n",
      "Trainable params: 8,486,090\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create CNN model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n",
    "model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 44s 580ms/step - loss: 0.7069 - accuracy: 0.7908 - val_loss: 3.4804 - val_accuracy: 0.1150\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 25s 338ms/step - loss: 0.2231 - accuracy: 0.9388 - val_loss: 3.6891 - val_accuracy: 0.0983\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 25s 327ms/step - loss: 0.1478 - accuracy: 0.9563 - val_loss: 4.6165 - val_accuracy: 0.1133\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 25s 338ms/step - loss: 0.1035 - accuracy: 0.9700 - val_loss: 2.6267 - val_accuracy: 0.3100\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 27s 365ms/step - loss: 0.0674 - accuracy: 0.9837 - val_loss: 1.0832 - val_accuracy: 0.6517\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 27s 362ms/step - loss: 0.0435 - accuracy: 0.9892 - val_loss: 0.4339 - val_accuracy: 0.8533\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 25s 330ms/step - loss: 0.0286 - accuracy: 0.9942 - val_loss: 0.1773 - val_accuracy: 0.9450\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 27s 353ms/step - loss: 0.0228 - accuracy: 0.9958 - val_loss: 0.0888 - val_accuracy: 0.9733\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 25s 335ms/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 0.1145 - val_accuracy: 0.9600\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 29s 393ms/step - loss: 0.0137 - accuracy: 0.9979 - val_loss: 0.3686 - val_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Train model for 10 epochs, capture the history\n",
    "history = model.fit(train_dataset, \n",
    "                    epochs=10, \n",
    "                    validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"models/model2_checkpoint\"\n",
    "\n",
    "model.save_weights(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
